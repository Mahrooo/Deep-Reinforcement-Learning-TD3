{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3_Ant.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HAHMB0Ze8fU0",
        "outputId": "f2a39ba2-4a9b-49ce-dc38-28784a363d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install pybullet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pybullet in /usr/local/lib/python3.6/dist-packages (2.7.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ikr2p0Js8iB4",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u5rW0IDB8nTO",
        "colab": {}
      },
      "source": [
        "#nitialize the Experience Replay memory\n",
        "class ReplayBuffer(object): #does not inheret from other classes\n",
        "\n",
        "  def __init__(self, max_size=1e6):#input:capacity of the memory (maximum transition can be stored)\n",
        "    self.storage = []#memory\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0#pointer:different index of the cell of memory (add transition to memory and sampling)\n",
        "\n",
        "  def add(self, transition):#add transition to the memory\n",
        "    if len(self.storage) == self.max_size:# to check if the memory is full\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else: # memory is not fully populated\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "     #batch_dones=1 if the episode is completed otherwise it's 0 \n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind: \n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "      #transpose reward and dones \n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4CeRW4D79HL0",
        "colab": {}
      },
      "source": [
        "#nitialize the Experience Replay memory\n",
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):#dim of input of NN, dim of output, to clip the action in certaion range\n",
        "    super(Actor, self).__init__()#activating the inheritage with super function. \n",
        "    self.h1 = nn.Linear(state_dim, 400)\n",
        "    self.h2 = nn.Linear(400, 300)\n",
        "    self.h3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action#for cliping the actions\n",
        "\n",
        "  def forward(self, x):#forward propagation x:the input state. \n",
        "    x = F.relu(self.h1(x))\n",
        "    x = F.relu(self.h2(x))\n",
        "    x = self.max_action * torch.tanh(self.h3(x))#value between (-1,1), to go back to original action value. \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OCee7gwR9Jrs",
        "colab": {}
      },
      "source": [
        "#build two neural networks for the two Critic models and two neural networks for the two Critic targets\n",
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):#dim of input of NN, dim of output, to clip the action in certaion range\n",
        "    super(Critic, self).__init__()\n",
        "    # first critic neuron:\n",
        "    self.h1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.h2 = nn.Linear(400, 300)\n",
        "    self.h3 = nn.Linear(300, 1)\n",
        "    #second critic neuron:\n",
        "    self.h4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.h5 = nn.Linear(400, 300)\n",
        "    self.h6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):#forward propagation x:the input state. concatinate of action and state\n",
        "    xu = torch.cat([x, u], 1)#vertically\n",
        "    x1 = F.relu(self.h1(xu))\n",
        "    x1 = F.relu(self.h2(x1))\n",
        "    x1 = self.h3(x1)\n",
        "    \n",
        "    x2 = F.relu(self.h4(xu))\n",
        "    x2 = F.relu(self.h5(x2))\n",
        "    x2 = self.h6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.h1(xu))\n",
        "    x1 = F.relu(self.h2(x1))\n",
        "    x1 = self.h3(x1)\n",
        "    return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zzd0H1xukdKe",
        "colab": {}
      },
      "source": [
        "#Training Process\n",
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "class TD3(object):#training the agent over ceratin amount of time steps. \n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)#gradient descent\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)#poliac average\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())#load with parameters of actor model\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):#input of actor  \n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)#in horizontal vector\n",
        "    #convert back to numpy for adding noise and cliping\n",
        "    return self.actor(state).cpu().data.numpy().flatten()#to forward propagation we can use cpu. flattern:to get action in 1D array\n",
        "\n",
        " #do exploration we have policy noise, standard deviation!\n",
        "#policy_freq: frequency of the delay, update the target once every iteration. \n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "     #sample a batch of transitions (s, s’, a, r) from the memory, creat 4 seperate batches:\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      #From the next state s', the Actor target plays the next action a'\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      #add Gaussian noise to this next action a’ and we clip it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      #The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs:\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      #keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      #get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2)\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      #backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      #every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        #once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        #once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  #save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  #load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qabqiYdp9wDM",
        "colab": {}
      },
      "source": [
        "#make a function that evaluates the policy by calculating its average reward over 10 episodes\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HFj6wbAo97lk",
        "colab": {}
      },
      "source": [
        "env_name = \"Walker2DBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1fyH8N5z-o3o",
        "outputId": "30dbde74-ae07-460a-a513-2022712f0393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#create a file name for the two saved models: the Actor and Critic models\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_Walker2DBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Src07lvY-zXb",
        "colab": {}
      },
      "source": [
        "#create a folder inside which will be saved the trained models\n",
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CyQXJUIs-6BV",
        "outputId": "054da883-a899-4e3d-a418-8e257cfa03fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#create the PyBullet environment\n",
        "env = gym.make(env_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z3RufYec_ADj",
        "colab": {}
      },
      "source": [
        "#set seeds \n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wTVvG7F8_EWg",
        "colab": {}
      },
      "source": [
        "#create the policy network (the Actor model)\n",
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sd-ZsdXR_LgV",
        "colab": {}
      },
      "source": [
        "#create the Experience Replay memory\n",
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dhC_5XJ__Orp",
        "outputId": "1e05d4f9-813b-4083-e729-464b895fe882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#define a list where all the evaluation results over 10 episodes are stored\n",
        "evaluations = [evaluate_policy(policy)]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 264.967700\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MTL9uMd0ru03",
        "colab": {}
      },
      "source": [
        "#create a new folder directory in which the final results (videos of the agent) will be populated\n",
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1vN5EvxK_QhT",
        "colab": {}
      },
      "source": [
        "#initialize the variables\n",
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y_ouY4NH_Y0I",
        "outputId": "976590b7-d878-4637-ce84-13403117041a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Training\n",
        "# We start the main loop over 500,000 timesteps\n",
        "c=0\n",
        "R=np.array([])\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        " \n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "  \n",
        "  # We check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  \n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 11 Episode Num: 1 Reward: 15.088533259392717\n",
            "Total Timesteps: 22 Episode Num: 2 Reward: 14.470926079206404\n",
            "Total Timesteps: 35 Episode Num: 3 Reward: 17.117014334668056\n",
            "Total Timesteps: 53 Episode Num: 4 Reward: 16.661888677394018\n",
            "Total Timesteps: 69 Episode Num: 5 Reward: 18.690938759343407\n",
            "Total Timesteps: 79 Episode Num: 6 Reward: 14.348573336953995\n",
            "Total Timesteps: 93 Episode Num: 7 Reward: 18.297878933146418\n",
            "Total Timesteps: 101 Episode Num: 8 Reward: 13.737867637064483\n",
            "Total Timesteps: 116 Episode Num: 9 Reward: 18.080415860390346\n",
            "Total Timesteps: 123 Episode Num: 10 Reward: 12.186578871974778\n",
            "Total Timesteps: 129 Episode Num: 11 Reward: 11.134814114941399\n",
            "Total Timesteps: 141 Episode Num: 12 Reward: 16.440621967866903\n",
            "Total Timesteps: 158 Episode Num: 13 Reward: 19.838173702383937\n",
            "Total Timesteps: 167 Episode Num: 14 Reward: 13.826522648530954\n",
            "Total Timesteps: 184 Episode Num: 15 Reward: 17.341771677632643\n",
            "Total Timesteps: 192 Episode Num: 16 Reward: 11.584601893951184\n",
            "Total Timesteps: 208 Episode Num: 17 Reward: 18.376173329564335\n",
            "Total Timesteps: 219 Episode Num: 18 Reward: 15.703786640349426\n",
            "Total Timesteps: 227 Episode Num: 19 Reward: 13.645377270496102\n",
            "Total Timesteps: 250 Episode Num: 20 Reward: 21.98717378048314\n",
            "Total Timesteps: 264 Episode Num: 21 Reward: 17.691737900332374\n",
            "Total Timesteps: 281 Episode Num: 22 Reward: 19.816722218773794\n",
            "Total Timesteps: 296 Episode Num: 23 Reward: 16.313753800351694\n",
            "Total Timesteps: 311 Episode Num: 24 Reward: 14.67493414754863\n",
            "Total Timesteps: 327 Episode Num: 25 Reward: 20.066179125553752\n",
            "Total Timesteps: 341 Episode Num: 26 Reward: 12.32831577429315\n",
            "Total Timesteps: 353 Episode Num: 27 Reward: 16.734820864324867\n",
            "Total Timesteps: 364 Episode Num: 28 Reward: 14.231604370224522\n",
            "Total Timesteps: 380 Episode Num: 29 Reward: 17.09317714313365\n",
            "Total Timesteps: 398 Episode Num: 30 Reward: 18.418955931319214\n",
            "Total Timesteps: 405 Episode Num: 31 Reward: 12.366777458296566\n",
            "Total Timesteps: 416 Episode Num: 32 Reward: 15.169979989467539\n",
            "Total Timesteps: 427 Episode Num: 33 Reward: 13.978052777703853\n",
            "Total Timesteps: 437 Episode Num: 34 Reward: 13.899458350245549\n",
            "Total Timesteps: 466 Episode Num: 35 Reward: 26.64134714846296\n",
            "Total Timesteps: 475 Episode Num: 36 Reward: 13.782592405665492\n",
            "Total Timesteps: 490 Episode Num: 37 Reward: 19.667874077460144\n",
            "Total Timesteps: 519 Episode Num: 38 Reward: 30.80485010012344\n",
            "Total Timesteps: 530 Episode Num: 39 Reward: 12.626817843892788\n",
            "Total Timesteps: 544 Episode Num: 40 Reward: 16.024319481854036\n",
            "Total Timesteps: 575 Episode Num: 41 Reward: 27.905054163991007\n",
            "Total Timesteps: 589 Episode Num: 42 Reward: 17.757247671759977\n",
            "Total Timesteps: 604 Episode Num: 43 Reward: 16.852608225481525\n",
            "Total Timesteps: 624 Episode Num: 44 Reward: 19.863330688000133\n",
            "Total Timesteps: 633 Episode Num: 45 Reward: 14.604875977209304\n",
            "Total Timesteps: 644 Episode Num: 46 Reward: 15.1815685907437\n",
            "Total Timesteps: 654 Episode Num: 47 Reward: 15.028389877825976\n",
            "Total Timesteps: 686 Episode Num: 48 Reward: 28.713026670592082\n",
            "Total Timesteps: 693 Episode Num: 49 Reward: 11.498049394815462\n",
            "Total Timesteps: 702 Episode Num: 50 Reward: 14.011949104983067\n",
            "Total Timesteps: 718 Episode Num: 51 Reward: 17.447999855241505\n",
            "Total Timesteps: 730 Episode Num: 52 Reward: 14.447079208859941\n",
            "Total Timesteps: 754 Episode Num: 53 Reward: 24.856231645471414\n",
            "Total Timesteps: 762 Episode Num: 54 Reward: 12.906863577930197\n",
            "Total Timesteps: 772 Episode Num: 55 Reward: 13.978920334897701\n",
            "Total Timesteps: 782 Episode Num: 56 Reward: 14.285353721643332\n",
            "Total Timesteps: 795 Episode Num: 57 Reward: 15.59073212057265\n",
            "Total Timesteps: 811 Episode Num: 58 Reward: 19.114008075589663\n",
            "Total Timesteps: 828 Episode Num: 59 Reward: 19.853430525798466\n",
            "Total Timesteps: 843 Episode Num: 60 Reward: 18.45209716834361\n",
            "Total Timesteps: 861 Episode Num: 61 Reward: 16.948657014370838\n",
            "Total Timesteps: 879 Episode Num: 62 Reward: 20.508151799337064\n",
            "Total Timesteps: 894 Episode Num: 63 Reward: 18.033956247748577\n",
            "Total Timesteps: 912 Episode Num: 64 Reward: 18.009807825188908\n",
            "Total Timesteps: 923 Episode Num: 65 Reward: 13.862743649599723\n",
            "Total Timesteps: 932 Episode Num: 66 Reward: 14.273880562622798\n",
            "Total Timesteps: 939 Episode Num: 67 Reward: 11.952743304638716\n",
            "Total Timesteps: 949 Episode Num: 68 Reward: 14.800233760554692\n",
            "Total Timesteps: 958 Episode Num: 69 Reward: 13.72258350789634\n",
            "Total Timesteps: 977 Episode Num: 70 Reward: 18.368499027105283\n",
            "Total Timesteps: 987 Episode Num: 71 Reward: 15.33585714508372\n",
            "Total Timesteps: 994 Episode Num: 72 Reward: 11.835887783116778\n",
            "Total Timesteps: 1014 Episode Num: 73 Reward: 23.431590338767275\n",
            "Total Timesteps: 1030 Episode Num: 74 Reward: 16.75679852519388\n",
            "Total Timesteps: 1051 Episode Num: 75 Reward: 22.71265307267313\n",
            "Total Timesteps: 1071 Episode Num: 76 Reward: 20.076714409312988\n",
            "Total Timesteps: 1085 Episode Num: 77 Reward: 13.915898821331211\n",
            "Total Timesteps: 1095 Episode Num: 78 Reward: 14.044828953116664\n",
            "Total Timesteps: 1105 Episode Num: 79 Reward: 15.07555943095358\n",
            "Total Timesteps: 1128 Episode Num: 80 Reward: 21.084485005572784\n",
            "Total Timesteps: 1147 Episode Num: 81 Reward: 14.860372493037723\n",
            "Total Timesteps: 1164 Episode Num: 82 Reward: 20.006689797362196\n",
            "Total Timesteps: 1179 Episode Num: 83 Reward: 18.441454827247068\n",
            "Total Timesteps: 1198 Episode Num: 84 Reward: 20.992624341872578\n",
            "Total Timesteps: 1211 Episode Num: 85 Reward: 16.503187436256848\n",
            "Total Timesteps: 1225 Episode Num: 86 Reward: 17.88753018815769\n",
            "Total Timesteps: 1237 Episode Num: 87 Reward: 16.317876638291636\n",
            "Total Timesteps: 1263 Episode Num: 88 Reward: 19.230484251733277\n",
            "Total Timesteps: 1279 Episode Num: 89 Reward: 18.239554828249677\n",
            "Total Timesteps: 1289 Episode Num: 90 Reward: 14.91431303759309\n",
            "Total Timesteps: 1307 Episode Num: 91 Reward: 17.716806035200715\n",
            "Total Timesteps: 1328 Episode Num: 92 Reward: 20.53581052726659\n",
            "Total Timesteps: 1340 Episode Num: 93 Reward: 15.154003832090531\n",
            "Total Timesteps: 1349 Episode Num: 94 Reward: 13.636140558768238\n",
            "Total Timesteps: 1366 Episode Num: 95 Reward: 19.84942537956667\n",
            "Total Timesteps: 1383 Episode Num: 96 Reward: 18.072197764927115\n",
            "Total Timesteps: 1395 Episode Num: 97 Reward: 16.04083871558978\n",
            "Total Timesteps: 1406 Episode Num: 98 Reward: 12.007866905310948\n",
            "Total Timesteps: 1432 Episode Num: 99 Reward: 23.99525938723236\n",
            "Total Timesteps: 1443 Episode Num: 100 Reward: 15.915488405671201\n",
            "Total Timesteps: 1454 Episode Num: 101 Reward: 16.02080065621703\n",
            "Total Timesteps: 1464 Episode Num: 102 Reward: 13.516330795525572\n",
            "Total Timesteps: 1474 Episode Num: 103 Reward: 12.333605621154128\n",
            "Total Timesteps: 1486 Episode Num: 104 Reward: 13.702463284174156\n",
            "Total Timesteps: 1498 Episode Num: 105 Reward: 14.874539099476532\n",
            "Total Timesteps: 1511 Episode Num: 106 Reward: 15.907035279134286\n",
            "Total Timesteps: 1532 Episode Num: 107 Reward: 21.851758506841723\n",
            "Total Timesteps: 1554 Episode Num: 108 Reward: 19.927591388265135\n",
            "Total Timesteps: 1562 Episode Num: 109 Reward: 13.870963272759399\n",
            "Total Timesteps: 1574 Episode Num: 110 Reward: 17.1925536470575\n",
            "Total Timesteps: 1588 Episode Num: 111 Reward: 16.31257232223288\n",
            "Total Timesteps: 1599 Episode Num: 112 Reward: 13.659816382057032\n",
            "Total Timesteps: 1624 Episode Num: 113 Reward: 27.313132031257553\n",
            "Total Timesteps: 1641 Episode Num: 114 Reward: 18.978073876636337\n",
            "Total Timesteps: 1653 Episode Num: 115 Reward: 16.550191426645323\n",
            "Total Timesteps: 1662 Episode Num: 116 Reward: 14.328719376098888\n",
            "Total Timesteps: 1689 Episode Num: 117 Reward: 24.174298696193727\n",
            "Total Timesteps: 1706 Episode Num: 118 Reward: 17.55302823189122\n",
            "Total Timesteps: 1718 Episode Num: 119 Reward: 15.156972778185446\n",
            "Total Timesteps: 1735 Episode Num: 120 Reward: 18.21703604972281\n",
            "Total Timesteps: 1746 Episode Num: 121 Reward: 13.701635636188438\n",
            "Total Timesteps: 1761 Episode Num: 122 Reward: 11.726898792885189\n",
            "Total Timesteps: 1776 Episode Num: 123 Reward: 15.815812898338482\n",
            "Total Timesteps: 1790 Episode Num: 124 Reward: 18.25668006688211\n",
            "Total Timesteps: 1804 Episode Num: 125 Reward: 17.196520192699975\n",
            "Total Timesteps: 1818 Episode Num: 126 Reward: 18.27577503993234\n",
            "Total Timesteps: 1832 Episode Num: 127 Reward: 16.170233822359297\n",
            "Total Timesteps: 1853 Episode Num: 128 Reward: 18.4231049958762\n",
            "Total Timesteps: 1866 Episode Num: 129 Reward: 16.5343888322328\n",
            "Total Timesteps: 1893 Episode Num: 130 Reward: 20.721464836924866\n",
            "Total Timesteps: 1903 Episode Num: 131 Reward: 14.888571756277814\n",
            "Total Timesteps: 1930 Episode Num: 132 Reward: 24.538000460503103\n",
            "Total Timesteps: 1947 Episode Num: 133 Reward: 19.124874895604446\n",
            "Total Timesteps: 1955 Episode Num: 134 Reward: 13.92818839634565\n",
            "Total Timesteps: 1967 Episode Num: 135 Reward: 13.819020903883208\n",
            "Total Timesteps: 1975 Episode Num: 136 Reward: 12.385753904865123\n",
            "Total Timesteps: 1984 Episode Num: 137 Reward: 13.26316132676584\n",
            "Total Timesteps: 1996 Episode Num: 138 Reward: 16.1075702513961\n",
            "Total Timesteps: 2003 Episode Num: 139 Reward: 12.94482076504064\n",
            "Total Timesteps: 2014 Episode Num: 140 Reward: 14.79143315104884\n",
            "Total Timesteps: 2027 Episode Num: 141 Reward: 15.086794260608437\n",
            "Total Timesteps: 2037 Episode Num: 142 Reward: 14.352847358572761\n",
            "Total Timesteps: 2044 Episode Num: 143 Reward: 11.998065473778116\n",
            "Total Timesteps: 2076 Episode Num: 144 Reward: 27.704729819236675\n",
            "Total Timesteps: 2086 Episode Num: 145 Reward: 14.552689019736135\n",
            "Total Timesteps: 2101 Episode Num: 146 Reward: 18.984422204064327\n",
            "Total Timesteps: 2110 Episode Num: 147 Reward: 13.816028763516808\n",
            "Total Timesteps: 2120 Episode Num: 148 Reward: 13.197141886515602\n",
            "Total Timesteps: 2138 Episode Num: 149 Reward: 19.109144994943925\n",
            "Total Timesteps: 2151 Episode Num: 150 Reward: 15.73833974646841\n",
            "Total Timesteps: 2163 Episode Num: 151 Reward: 15.424183007955438\n",
            "Total Timesteps: 2184 Episode Num: 152 Reward: 21.621679521629996\n",
            "Total Timesteps: 2194 Episode Num: 153 Reward: 14.204361041772062\n",
            "Total Timesteps: 2209 Episode Num: 154 Reward: 15.92798810543463\n",
            "Total Timesteps: 2228 Episode Num: 155 Reward: 20.932020835785078\n",
            "Total Timesteps: 2244 Episode Num: 156 Reward: 18.946416245891307\n",
            "Total Timesteps: 2252 Episode Num: 157 Reward: 12.128315453141113\n",
            "Total Timesteps: 2264 Episode Num: 158 Reward: 12.380447426588216\n",
            "Total Timesteps: 2284 Episode Num: 159 Reward: 18.84113702737086\n",
            "Total Timesteps: 2295 Episode Num: 160 Reward: 15.211655574491306\n",
            "Total Timesteps: 2320 Episode Num: 161 Reward: 27.019941020420813\n",
            "Total Timesteps: 2329 Episode Num: 162 Reward: 14.16040817987523\n",
            "Total Timesteps: 2349 Episode Num: 163 Reward: 19.933802471535454\n",
            "Total Timesteps: 2366 Episode Num: 164 Reward: 19.566079631327014\n",
            "Total Timesteps: 2379 Episode Num: 165 Reward: 14.969448510413347\n",
            "Total Timesteps: 2391 Episode Num: 166 Reward: 13.973057709814746\n",
            "Total Timesteps: 2405 Episode Num: 167 Reward: 19.170768840154054\n",
            "Total Timesteps: 2424 Episode Num: 168 Reward: 17.496828174260735\n",
            "Total Timesteps: 2438 Episode Num: 169 Reward: 16.645451554218017\n",
            "Total Timesteps: 2455 Episode Num: 170 Reward: 17.323421539139236\n",
            "Total Timesteps: 2469 Episode Num: 171 Reward: 15.18286900908715\n",
            "Total Timesteps: 2486 Episode Num: 172 Reward: 19.98789745506365\n",
            "Total Timesteps: 2499 Episode Num: 173 Reward: 17.18342853063077\n",
            "Total Timesteps: 2512 Episode Num: 174 Reward: 16.40443012891192\n",
            "Total Timesteps: 2532 Episode Num: 175 Reward: 21.0696177647711\n",
            "Total Timesteps: 2542 Episode Num: 176 Reward: 13.50546963556408\n",
            "Total Timesteps: 2559 Episode Num: 177 Reward: 19.000119695242027\n",
            "Total Timesteps: 2595 Episode Num: 178 Reward: 35.772405426492334\n",
            "Total Timesteps: 2605 Episode Num: 179 Reward: 14.90759011165792\n",
            "Total Timesteps: 2615 Episode Num: 180 Reward: 13.0273583546179\n",
            "Total Timesteps: 2624 Episode Num: 181 Reward: 15.274464226140116\n",
            "Total Timesteps: 2636 Episode Num: 182 Reward: 15.688479820737845\n",
            "Total Timesteps: 2644 Episode Num: 183 Reward: 12.15610586314142\n",
            "Total Timesteps: 2661 Episode Num: 184 Reward: 21.549311274448698\n",
            "Total Timesteps: 2669 Episode Num: 185 Reward: 12.246980941128276\n",
            "Total Timesteps: 2686 Episode Num: 186 Reward: 18.501945878782138\n",
            "Total Timesteps: 2696 Episode Num: 187 Reward: 15.080790199352487\n",
            "Total Timesteps: 2709 Episode Num: 188 Reward: 15.14381334745267\n",
            "Total Timesteps: 2726 Episode Num: 189 Reward: 16.505500762007433\n",
            "Total Timesteps: 2734 Episode Num: 190 Reward: 12.917391808351384\n",
            "Total Timesteps: 2742 Episode Num: 191 Reward: 13.22654946209659\n",
            "Total Timesteps: 2754 Episode Num: 192 Reward: 14.831240976823027\n",
            "Total Timesteps: 2765 Episode Num: 193 Reward: 13.940483429127196\n",
            "Total Timesteps: 2780 Episode Num: 194 Reward: 15.545612554131365\n",
            "Total Timesteps: 2801 Episode Num: 195 Reward: 19.644314726942685\n",
            "Total Timesteps: 2814 Episode Num: 196 Reward: 17.22064784680697\n",
            "Total Timesteps: 2827 Episode Num: 197 Reward: 16.292588811379392\n",
            "Total Timesteps: 2843 Episode Num: 198 Reward: 17.935879633802685\n",
            "Total Timesteps: 2852 Episode Num: 199 Reward: 14.353940560245247\n",
            "Total Timesteps: 2865 Episode Num: 200 Reward: 17.241509860038057\n",
            "Total Timesteps: 2877 Episode Num: 201 Reward: 16.02340638165915\n",
            "Total Timesteps: 2894 Episode Num: 202 Reward: 18.913593843705893\n",
            "Total Timesteps: 2910 Episode Num: 203 Reward: 16.514114750918814\n",
            "Total Timesteps: 2921 Episode Num: 204 Reward: 14.12165539217094\n",
            "Total Timesteps: 2944 Episode Num: 205 Reward: 20.6808589118562\n",
            "Total Timesteps: 2959 Episode Num: 206 Reward: 18.886074208059288\n",
            "Total Timesteps: 2971 Episode Num: 207 Reward: 13.788387081175461\n",
            "Total Timesteps: 2979 Episode Num: 208 Reward: 11.976434665465785\n",
            "Total Timesteps: 2996 Episode Num: 209 Reward: 16.99826373411488\n",
            "Total Timesteps: 3009 Episode Num: 210 Reward: 16.099521188871584\n",
            "Total Timesteps: 3020 Episode Num: 211 Reward: 15.57531965515809\n",
            "Total Timesteps: 3040 Episode Num: 212 Reward: 19.927405472515968\n",
            "Total Timesteps: 3054 Episode Num: 213 Reward: 16.747821584515624\n",
            "Total Timesteps: 3075 Episode Num: 214 Reward: 19.40609112775564\n",
            "Total Timesteps: 3095 Episode Num: 215 Reward: 21.88649719309469\n",
            "Total Timesteps: 3104 Episode Num: 216 Reward: 13.11529695476347\n",
            "Total Timesteps: 3120 Episode Num: 217 Reward: 16.938585820241133\n",
            "Total Timesteps: 3136 Episode Num: 218 Reward: 16.91625938152283\n",
            "Total Timesteps: 3145 Episode Num: 219 Reward: 13.405143199414304\n",
            "Total Timesteps: 3157 Episode Num: 220 Reward: 15.003249337118177\n",
            "Total Timesteps: 3166 Episode Num: 221 Reward: 13.101715960177534\n",
            "Total Timesteps: 3180 Episode Num: 222 Reward: 14.806259656020849\n",
            "Total Timesteps: 3190 Episode Num: 223 Reward: 15.435180851460606\n",
            "Total Timesteps: 3207 Episode Num: 224 Reward: 18.323084760594067\n",
            "Total Timesteps: 3215 Episode Num: 225 Reward: 13.170608014432945\n",
            "Total Timesteps: 3231 Episode Num: 226 Reward: 11.765206755101097\n",
            "Total Timesteps: 3253 Episode Num: 227 Reward: 20.76534421876422\n",
            "Total Timesteps: 3278 Episode Num: 228 Reward: 23.385048984922463\n",
            "Total Timesteps: 3287 Episode Num: 229 Reward: 13.128010245804035\n",
            "Total Timesteps: 3307 Episode Num: 230 Reward: 14.92815170264366\n",
            "Total Timesteps: 3325 Episode Num: 231 Reward: 18.155393330422516\n",
            "Total Timesteps: 3343 Episode Num: 232 Reward: 20.623914143802537\n",
            "Total Timesteps: 3361 Episode Num: 233 Reward: 18.398135634472418\n",
            "Total Timesteps: 3380 Episode Num: 234 Reward: 18.224091917734764\n",
            "Total Timesteps: 3395 Episode Num: 235 Reward: 18.23700626761565\n",
            "Total Timesteps: 3404 Episode Num: 236 Reward: 13.088477083880571\n",
            "Total Timesteps: 3415 Episode Num: 237 Reward: 14.016927453069364\n",
            "Total Timesteps: 3440 Episode Num: 238 Reward: 24.17639720255683\n",
            "Total Timesteps: 3458 Episode Num: 239 Reward: 17.89920956526912\n",
            "Total Timesteps: 3474 Episode Num: 240 Reward: 16.531412690656726\n",
            "Total Timesteps: 3486 Episode Num: 241 Reward: 17.145085148377987\n",
            "Total Timesteps: 3500 Episode Num: 242 Reward: 16.971964740022667\n",
            "Total Timesteps: 3512 Episode Num: 243 Reward: 14.63515031021816\n",
            "Total Timesteps: 3539 Episode Num: 244 Reward: 23.782299351785333\n",
            "Total Timesteps: 3558 Episode Num: 245 Reward: 17.41819127693743\n",
            "Total Timesteps: 3570 Episode Num: 246 Reward: 16.616967750106415\n",
            "Total Timesteps: 3593 Episode Num: 247 Reward: 22.557422529073666\n",
            "Total Timesteps: 3604 Episode Num: 248 Reward: 16.35413349829614\n",
            "Total Timesteps: 3616 Episode Num: 249 Reward: 15.601130896896938\n",
            "Total Timesteps: 3630 Episode Num: 250 Reward: 15.250785449724932\n",
            "Total Timesteps: 3646 Episode Num: 251 Reward: 19.48012419331499\n",
            "Total Timesteps: 3662 Episode Num: 252 Reward: 16.424169320659715\n",
            "Total Timesteps: 3671 Episode Num: 253 Reward: 13.229304860786941\n",
            "Total Timesteps: 3691 Episode Num: 254 Reward: 19.299237700595405\n",
            "Total Timesteps: 3702 Episode Num: 255 Reward: 13.853177001846777\n",
            "Total Timesteps: 3718 Episode Num: 256 Reward: 19.283977213820616\n",
            "Total Timesteps: 3742 Episode Num: 257 Reward: 19.341239475359906\n",
            "Total Timesteps: 3752 Episode Num: 258 Reward: 13.369857138941006\n",
            "Total Timesteps: 3770 Episode Num: 259 Reward: 20.86384286730754\n",
            "Total Timesteps: 3781 Episode Num: 260 Reward: 15.373142062193073\n",
            "Total Timesteps: 3791 Episode Num: 261 Reward: 13.999180749632067\n",
            "Total Timesteps: 3803 Episode Num: 262 Reward: 15.813842807667971\n",
            "Total Timesteps: 3824 Episode Num: 263 Reward: 21.381778870640847\n",
            "Total Timesteps: 3832 Episode Num: 264 Reward: 12.529711247279192\n",
            "Total Timesteps: 3841 Episode Num: 265 Reward: 11.605832843911775\n",
            "Total Timesteps: 3855 Episode Num: 266 Reward: 17.938870536060133\n",
            "Total Timesteps: 3874 Episode Num: 267 Reward: 17.430389449484935\n",
            "Total Timesteps: 3891 Episode Num: 268 Reward: 19.13369931781053\n",
            "Total Timesteps: 3898 Episode Num: 269 Reward: 11.11493320992304\n",
            "Total Timesteps: 3906 Episode Num: 270 Reward: 12.180706903757528\n",
            "Total Timesteps: 3919 Episode Num: 271 Reward: 15.142000460196865\n",
            "Total Timesteps: 3930 Episode Num: 272 Reward: 15.469256181636592\n",
            "Total Timesteps: 3940 Episode Num: 273 Reward: 13.351531116118712\n",
            "Total Timesteps: 3952 Episode Num: 274 Reward: 15.377425038629733\n",
            "Total Timesteps: 3979 Episode Num: 275 Reward: 21.528662635978254\n",
            "Total Timesteps: 3995 Episode Num: 276 Reward: 16.908194352728604\n",
            "Total Timesteps: 4015 Episode Num: 277 Reward: 25.379713565265416\n",
            "Total Timesteps: 4044 Episode Num: 278 Reward: 23.511828370093998\n",
            "Total Timesteps: 4055 Episode Num: 279 Reward: 14.788117097209033\n",
            "Total Timesteps: 4066 Episode Num: 280 Reward: 15.88041725318326\n",
            "Total Timesteps: 4077 Episode Num: 281 Reward: 14.985949678982433\n",
            "Total Timesteps: 4088 Episode Num: 282 Reward: 13.936396771811998\n",
            "Total Timesteps: 4103 Episode Num: 283 Reward: 16.02527982878819\n",
            "Total Timesteps: 4117 Episode Num: 284 Reward: 16.0451496974958\n",
            "Total Timesteps: 4128 Episode Num: 285 Reward: 16.705373943020824\n",
            "Total Timesteps: 4163 Episode Num: 286 Reward: 25.462976475259342\n",
            "Total Timesteps: 4174 Episode Num: 287 Reward: 13.927529272047103\n",
            "Total Timesteps: 4190 Episode Num: 288 Reward: 16.923097404045983\n",
            "Total Timesteps: 4201 Episode Num: 289 Reward: 14.469858027742882\n",
            "Total Timesteps: 4210 Episode Num: 290 Reward: 13.922045296948633\n",
            "Total Timesteps: 4225 Episode Num: 291 Reward: 17.758422798795795\n",
            "Total Timesteps: 4237 Episode Num: 292 Reward: 16.227118221923593\n",
            "Total Timesteps: 4244 Episode Num: 293 Reward: 12.38954442358372\n",
            "Total Timesteps: 4257 Episode Num: 294 Reward: 16.455756922386357\n",
            "Total Timesteps: 4269 Episode Num: 295 Reward: 17.390396408204104\n",
            "Total Timesteps: 4284 Episode Num: 296 Reward: 17.60989108972717\n",
            "Total Timesteps: 4301 Episode Num: 297 Reward: 17.718646910287497\n",
            "Total Timesteps: 4311 Episode Num: 298 Reward: 13.905401875750977\n",
            "Total Timesteps: 4330 Episode Num: 299 Reward: 20.855049964437786\n",
            "Total Timesteps: 4339 Episode Num: 300 Reward: 12.202441889894544\n",
            "Total Timesteps: 4349 Episode Num: 301 Reward: 14.004968199593712\n",
            "Total Timesteps: 4359 Episode Num: 302 Reward: 13.200034757932006\n",
            "Total Timesteps: 4373 Episode Num: 303 Reward: 16.73864573059691\n",
            "Total Timesteps: 4385 Episode Num: 304 Reward: 15.947852884051096\n",
            "Total Timesteps: 4399 Episode Num: 305 Reward: 16.721035219094485\n",
            "Total Timesteps: 4415 Episode Num: 306 Reward: 11.754769069762553\n",
            "Total Timesteps: 4428 Episode Num: 307 Reward: 16.357929924290513\n",
            "Total Timesteps: 4439 Episode Num: 308 Reward: 15.297741416169444\n",
            "Total Timesteps: 4456 Episode Num: 309 Reward: 19.703350671460793\n",
            "Total Timesteps: 4465 Episode Num: 310 Reward: 13.728781351268116\n",
            "Total Timesteps: 4480 Episode Num: 311 Reward: 17.27932795827801\n",
            "Total Timesteps: 4497 Episode Num: 312 Reward: 17.61394230821024\n",
            "Total Timesteps: 4522 Episode Num: 313 Reward: 23.131260305229805\n",
            "Total Timesteps: 4529 Episode Num: 314 Reward: 12.18005408202589\n",
            "Total Timesteps: 4545 Episode Num: 315 Reward: 16.79606718930445\n",
            "Total Timesteps: 4558 Episode Num: 316 Reward: 16.87170720510476\n",
            "Total Timesteps: 4573 Episode Num: 317 Reward: 16.872619782791293\n",
            "Total Timesteps: 4590 Episode Num: 318 Reward: 17.137375575289475\n",
            "Total Timesteps: 4599 Episode Num: 319 Reward: 15.209592412605705\n",
            "Total Timesteps: 4616 Episode Num: 320 Reward: 17.078297695580112\n",
            "Total Timesteps: 4635 Episode Num: 321 Reward: 20.28627242946386\n",
            "Total Timesteps: 4649 Episode Num: 322 Reward: 16.013178850345138\n",
            "Total Timesteps: 4657 Episode Num: 323 Reward: 12.457809511704545\n",
            "Total Timesteps: 4671 Episode Num: 324 Reward: 17.348459547461243\n",
            "Total Timesteps: 4682 Episode Num: 325 Reward: 16.855211519038132\n",
            "Total Timesteps: 4691 Episode Num: 326 Reward: 12.807274908720865\n",
            "Total Timesteps: 4707 Episode Num: 327 Reward: 18.524722508329432\n",
            "Total Timesteps: 4721 Episode Num: 328 Reward: 18.30473369621468\n",
            "Total Timesteps: 4733 Episode Num: 329 Reward: 14.101591498135532\n",
            "Total Timesteps: 4745 Episode Num: 330 Reward: 16.82743342084723\n",
            "Total Timesteps: 4760 Episode Num: 331 Reward: 17.627132302879183\n",
            "Total Timesteps: 4772 Episode Num: 332 Reward: 13.798545354779343\n",
            "Total Timesteps: 4781 Episode Num: 333 Reward: 14.48889931582089\n",
            "Total Timesteps: 4793 Episode Num: 334 Reward: 16.20831963398523\n",
            "Total Timesteps: 4805 Episode Num: 335 Reward: 15.549771996791241\n",
            "Total Timesteps: 4814 Episode Num: 336 Reward: 13.564344760091627\n",
            "Total Timesteps: 4831 Episode Num: 337 Reward: 18.45955415253993\n",
            "Total Timesteps: 4853 Episode Num: 338 Reward: 19.903054728228017\n",
            "Total Timesteps: 4875 Episode Num: 339 Reward: 19.59758242962125\n",
            "Total Timesteps: 4891 Episode Num: 340 Reward: 19.19025531104708\n",
            "Total Timesteps: 4900 Episode Num: 341 Reward: 13.774308405880582\n",
            "Total Timesteps: 4909 Episode Num: 342 Reward: 13.78597019727167\n",
            "Total Timesteps: 4923 Episode Num: 343 Reward: 17.413304391378187\n",
            "Total Timesteps: 4934 Episode Num: 344 Reward: 13.989062005408048\n",
            "Total Timesteps: 4942 Episode Num: 345 Reward: 13.000166763053857\n",
            "Total Timesteps: 4950 Episode Num: 346 Reward: 14.798817786478319\n",
            "Total Timesteps: 4966 Episode Num: 347 Reward: 17.092068960372124\n",
            "Total Timesteps: 4976 Episode Num: 348 Reward: 14.224047918751602\n",
            "Total Timesteps: 4988 Episode Num: 349 Reward: 13.962263617092686\n",
            "Total Timesteps: 4998 Episode Num: 350 Reward: 13.690834283060394\n",
            "Total Timesteps: 5011 Episode Num: 351 Reward: 15.942147237759489\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 3.275337\n",
            "---------------------------------------\n",
            "Total Timesteps: 5019 Episode Num: 352 Reward: 12.48327947707876\n",
            "Total Timesteps: 5036 Episode Num: 353 Reward: 18.678649806235626\n",
            "Total Timesteps: 5059 Episode Num: 354 Reward: 19.059449336839318\n",
            "Total Timesteps: 5072 Episode Num: 355 Reward: 17.793901709574858\n",
            "Total Timesteps: 5094 Episode Num: 356 Reward: 22.321869634784527\n",
            "Total Timesteps: 5111 Episode Num: 357 Reward: 19.24079960144445\n",
            "Total Timesteps: 5123 Episode Num: 358 Reward: 16.547996149260143\n",
            "Total Timesteps: 5144 Episode Num: 359 Reward: 19.051719009145746\n",
            "Total Timesteps: 5154 Episode Num: 360 Reward: 13.518971658677035\n",
            "Total Timesteps: 5169 Episode Num: 361 Reward: 17.52519310630887\n",
            "Total Timesteps: 5189 Episode Num: 362 Reward: 18.606291242499722\n",
            "Total Timesteps: 5208 Episode Num: 363 Reward: 20.145550368835387\n",
            "Total Timesteps: 5226 Episode Num: 364 Reward: 19.106151900657274\n",
            "Total Timesteps: 5235 Episode Num: 365 Reward: 13.5484664203992\n",
            "Total Timesteps: 5252 Episode Num: 366 Reward: 19.590754344796007\n",
            "Total Timesteps: 5262 Episode Num: 367 Reward: 14.502152152189227\n",
            "Total Timesteps: 5273 Episode Num: 368 Reward: 15.062450547168554\n",
            "Total Timesteps: 5293 Episode Num: 369 Reward: 21.2152148149471\n",
            "Total Timesteps: 5316 Episode Num: 370 Reward: 25.15162993390113\n",
            "Total Timesteps: 5326 Episode Num: 371 Reward: 14.522893011847916\n",
            "Total Timesteps: 5335 Episode Num: 372 Reward: 14.046208838307939\n",
            "Total Timesteps: 5357 Episode Num: 373 Reward: 22.910982609167696\n",
            "Total Timesteps: 5364 Episode Num: 374 Reward: 11.078485061651735\n",
            "Total Timesteps: 5374 Episode Num: 375 Reward: 15.558895464419038\n",
            "Total Timesteps: 5390 Episode Num: 376 Reward: 17.746531906219023\n",
            "Total Timesteps: 5400 Episode Num: 377 Reward: 14.070685248679364\n",
            "Total Timesteps: 5418 Episode Num: 378 Reward: 14.435956742613051\n",
            "Total Timesteps: 5429 Episode Num: 379 Reward: 13.996906922866764\n",
            "Total Timesteps: 5438 Episode Num: 380 Reward: 14.269896182382945\n",
            "Total Timesteps: 5457 Episode Num: 381 Reward: 18.800585009784847\n",
            "Total Timesteps: 5473 Episode Num: 382 Reward: 19.081887806668238\n",
            "Total Timesteps: 5487 Episode Num: 383 Reward: 17.2202380528106\n",
            "Total Timesteps: 5501 Episode Num: 384 Reward: 17.66617886228778\n",
            "Total Timesteps: 5513 Episode Num: 385 Reward: 16.22569796034222\n",
            "Total Timesteps: 5524 Episode Num: 386 Reward: 13.563650757691358\n",
            "Total Timesteps: 5531 Episode Num: 387 Reward: 13.127885875689389\n",
            "Total Timesteps: 5541 Episode Num: 388 Reward: 14.904592496171244\n",
            "Total Timesteps: 5558 Episode Num: 389 Reward: 16.72209042908216\n",
            "Total Timesteps: 5570 Episode Num: 390 Reward: 15.831766286799391\n",
            "Total Timesteps: 5580 Episode Num: 391 Reward: 14.055702081303751\n",
            "Total Timesteps: 5594 Episode Num: 392 Reward: 18.828183438163254\n",
            "Total Timesteps: 5613 Episode Num: 393 Reward: 17.425827014667448\n",
            "Total Timesteps: 5639 Episode Num: 394 Reward: 23.156072111389943\n",
            "Total Timesteps: 5646 Episode Num: 395 Reward: 12.544368849715099\n",
            "Total Timesteps: 5659 Episode Num: 396 Reward: 16.30963728289353\n",
            "Total Timesteps: 5672 Episode Num: 397 Reward: 15.547798184157\n",
            "Total Timesteps: 5681 Episode Num: 398 Reward: 12.373466891472345\n",
            "Total Timesteps: 5693 Episode Num: 399 Reward: 14.035475791689532\n",
            "Total Timesteps: 5715 Episode Num: 400 Reward: 19.303611096882374\n",
            "Total Timesteps: 5728 Episode Num: 401 Reward: 14.350952523430168\n",
            "Total Timesteps: 5748 Episode Num: 402 Reward: 19.990805680121415\n",
            "Total Timesteps: 5773 Episode Num: 403 Reward: 25.58332892722974\n",
            "Total Timesteps: 5790 Episode Num: 404 Reward: 17.202329045601076\n",
            "Total Timesteps: 5800 Episode Num: 405 Reward: 13.032856611840543\n",
            "Total Timesteps: 5809 Episode Num: 406 Reward: 14.352563557449324\n",
            "Total Timesteps: 5821 Episode Num: 407 Reward: 17.058986848467615\n",
            "Total Timesteps: 5832 Episode Num: 408 Reward: 14.747484116499255\n",
            "Total Timesteps: 5841 Episode Num: 409 Reward: 12.481752494754618\n",
            "Total Timesteps: 5856 Episode Num: 410 Reward: 16.941519789745506\n",
            "Total Timesteps: 5863 Episode Num: 411 Reward: 11.917842781939544\n",
            "Total Timesteps: 5880 Episode Num: 412 Reward: 18.10527176807227\n",
            "Total Timesteps: 5891 Episode Num: 413 Reward: 14.618093039635276\n",
            "Total Timesteps: 5905 Episode Num: 414 Reward: 16.69084486231295\n",
            "Total Timesteps: 5915 Episode Num: 415 Reward: 14.340208436454123\n",
            "Total Timesteps: 5934 Episode Num: 416 Reward: 17.96249477499514\n",
            "Total Timesteps: 5954 Episode Num: 417 Reward: 20.651442066476736\n",
            "Total Timesteps: 5979 Episode Num: 418 Reward: 23.904616362159143\n",
            "Total Timesteps: 5987 Episode Num: 419 Reward: 12.52742947624938\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e45dad08f735>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_timesteps\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total Timesteps: {} Episode Num: {} Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_clip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-1e9a4a277782>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, replay_buffer, iterations, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m      \u001b[0;31m#sample a batch of transitions (s, s’, a, r) from the memory, creat 4 seperate batches:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m       \u001b[0mbatch_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_next_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m       \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_next_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-d8f20f5fbb00>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m      \u001b[0;31m#batch_dones=1 if the episode is completed otherwise it's 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mbatch_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_next_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_bounded_integers.pyx\u001b[0m in \u001b[0;36mnumpy.random._bounded_integers._rand_int64\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prod_dispatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m def prod(a, axis=None, dtype=None, out=None, keepdims=np._NoValue,\n\u001b[1;32m   2845\u001b[0m          initial=np._NoValue, where=np._NoValue):\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0PBfVlxDr_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULHT2NvqDstH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n",
        "\n",
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward\n",
        "\n",
        "env_name = \"Walker2DBulletEnv-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}